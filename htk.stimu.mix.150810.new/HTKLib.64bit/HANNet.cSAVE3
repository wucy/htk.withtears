/* ----------------------------------------------------------- */
/*                                                             */
/*                          ___                                */
/*                       |_| | |_/   SPEECH                    */
/*                       | | | | \   RECOGNITION               */
/*                       =========   SOFTWARE                  */
/*                                                             */
/*                                                             */
/* ----------------------------------------------------------- */
/* developed at:                                               */
/*                                                             */
/*      Machine Intelligence Laboratory                        */
/*      Cambridge University Engineering Department            */
/*      http://mil.eng.cam.ac.uk/                              */
/*                                                             */
/*                                                             */
/* ----------------------------------------------------------- */
/*         Copyright: Microsoft Corporation                    */
/*          1995-2000 Redmond, Washington USA                  */
/*                    http://www.microsoft.com                 */
/*                                                             */
/*              2002  Cambridge University                     */
/*                    Engineering Department                   */
/*                                                             */
/*   Use of this software is governed by a License Agreement   */
/*    ** See the file License for the Conditions of Use  **    */
/*    **     This banner notice must not be removed      **    */
/*                                                             */
/* ----------------------------------------------------------- */
/*         File: HANNet.c  ANN Model Definition Data Type      */
/* ----------------------------------------------------------- */

char *hannet_version = "!HVER!HANNet:   3.4.1 [CUED 30/11/13]";
char *hannet_vc_id = "$Id: HANNet.c,v 1.1.1.1 2013/11/13 09:54:58 cz277 Exp $";

#include "cfgs.h"
#include <time.h>
#include "HShell.h"
#include "HMem.h"
#include "HMath.h"
#include "HSigP.h"
#include "HWave.h"
#include "HAudio.h"
#include "HParm.h"
#include "HLabel.h"
#include "HANNet.h"
#include "HModel.h"
#include "HTrain.h"
#include "HNet.h"
#include "HArc.h"
#include "HFBLat.h"
#include "HDict.h"
#include "HAdapt.h"
#include <math.h>

/* ------------------------------ Trace Flags ------------------------------ */

static int trace = 0;

#define T_TOP 0001
#define T_CCH 0002

/* --------------------------- Memory Management --------------------------- */


/* ----------------------------- Configuration ------------------------------*/

static ConfParam *cParm[MAXGLOBS];      /* config parameters */
static int nParm = 0;
static size_t batchSamples = 1;                 /* the number of samples in batch; 1 sample by default */
static char *updtFlagStr = NULL;                /* the string pointer indicating the layers to update */
static int updtIdx = 0;                         /* the index of current update*/
static Boolean hasShownUpdtFlag = FALSE;
/* cz277 - 1007 */
static int batIdx = 0;
/* cz277 - xform */
static RPLInfo *rplNMatInfo[MAXNUMRPLNMAT];
static RPLInfo *rplNVecInfo[MAXNUMRPLNVEC];

/* get the batch size */
int GetNBatchSamples(void) {
    return batchSamples;
}

/* set the batch size */
void SetNBatchSamples(int userBatchSamples) {
    batchSamples = userBatchSamples;
#ifdef CUDA
    RegisterTmpNMat(1, batchSamples);
#endif
}

/* set the index of current update */
void SetUpdateIndex(int curUpdtIdx) {
    updtIdx = curUpdtIdx;
}

/* get the index of current update */
int GetUpdateIndex(void) {
    return updtIdx;
}

/*  */
void SetBatchIndex(int curBatIdx) {
    batIdx = curBatIdx;
}

/*  */
int GetBatchIndex(void) {
    return batIdx;
}

/* cz277 - xform */
void InitRPLInfo(RPLInfo *rplInfo) {
    rplInfo->nSpkr = 0;
    rplInfo->inRPLMask = NULL;
    rplInfo->curOutSpkr = NewString(&gcheap, MAXSTRLEN);
    rplInfo->curInSpkr = NewString(&gcheap, MAXSTRLEN);
    rplInfo->cacheInSpkr = NewString(&gcheap, MAXSTRLEN);
    rplInfo->inRPLDir = NULL;
    rplInfo->inRPLExt = NULL;
    rplInfo->outRPLDir = NULL;
    rplInfo->outRPLExt = NULL;
    rplInfo->inActive = FALSE;
    rplInfo->outActive = FALSE;
    rplInfo->saveBinary = FALSE;
    rplInfo->rplNMat = NULL;
    memset(&rplInfo->saveRplNMatHost, 0, sizeof(NMatHost));
    /*rplInfo->saveRplNMat = NULL;*/
    rplInfo->rplNVec = NULL;
    memset(&rplInfo->saveRplNVecHost, 0, sizeof(NVecHost));
    /*rplInfo->saveRplNVec = NULL;*/
}

/*  */
void InitANNet(void)
{
    int intVal, i;
    char buf[MAXSTRLEN], cmd[MAXSTRLEN];

    Register(hannet_version, hannet_vc_id);
    nParm = GetConfig("HANNET", TRUE, cParm, MAXGLOBS);

    if (nParm > 0) {
        if (GetConfInt(cParm, nParm, "TRACE", &intVal)) { 
            trace = intVal;
        }
        if (GetConfInt(cParm, nParm, "BATCHSAMP", &intVal)) {
            if (intVal <= 0) {
                HError(9999, "InitANNet: Fail to set batch size");
            }
            SetNBatchSamples(intVal);
        }
        if (GetConfStr(cParm, nParm, "UPDATEFLAGS", buf)) {
            updtFlagStr = CopyString(&gcheap, buf);
        }
        /* cz277 - xform */
        for (i = 1; i < MAXNUMRPLNMAT; ++i) {
            rplNMatInfo[i] = NULL;
            /* mask */
            sprintf(cmd, "REPLACEABLENMATMASK%d", i);
            if (GetConfStr(cParm, nParm, cmd, buf)) {
                rplNMatInfo[i] = (RPLInfo *) New(&gcheap, sizeof(RPLInfo));
                InitRPLInfo(rplNMatInfo[i]);
                rplNMatInfo[i]->inRPLMask = CopyString(&gcheap, buf); 
            }
            if (rplNMatInfo[i] != NULL) {
                /* in dir */
                sprintf(cmd, "REPLACEABLENMATINDIR%d", i);
                if (GetConfStr(cParm, nParm, cmd, buf)) {
                    rplNMatInfo[i]->inRPLDir = CopyString(&gcheap, buf);
                }
                /* ext */
                sprintf(cmd, "REPLACEABLENMATEXT%d", i);
                if (GetConfStr(cParm, nParm, cmd, buf)) {
                    rplNMatInfo[i]->inRPLExt = CopyString(&gcheap, buf);
                    rplNMatInfo[i]->outRPLExt = CopyString(&gcheap, buf);
                }
                /* out dir */
                sprintf(cmd, "REPLACEABLENMATOUTDIR%d", i);
                if (GetConfStr(cParm, nParm, cmd, buf)) {
                    rplNMatInfo[i]->outRPLDir = CopyString(&gcheap, buf);
                }
            }
        }
        for (i = 1; i < MAXNUMRPLNVEC; ++i) {
            rplNVecInfo[i] = NULL;
            /* mask */
            sprintf(cmd, "REPLACEABLENVECMASK%d", i);
            if (GetConfStr(cParm, nParm, cmd, buf)) {
                rplNVecInfo[i] = (RPLInfo *) New(&gcheap, sizeof(RPLInfo));
                InitRPLInfo(rplNVecInfo[i]);
                rplNVecInfo[i]->inRPLMask = CopyString(&gcheap, buf);
            }
            if (rplNVecInfo[i] != NULL) {
                /* in dir */
                sprintf(cmd, "REPLACEABLENVECINDIR%d", i);
                if (GetConfStr(cParm, nParm, cmd, buf)) {
                    rplNVecInfo[i]->inRPLDir = CopyString(&gcheap, buf);
                }
                /* ext */
                sprintf(cmd, "REPLACEABLENVECEXT%d", i);
                if (GetConfStr(cParm, nParm, cmd, buf)) {
                    rplNVecInfo[i]->inRPLExt = CopyString(&gcheap, buf);
                    rplNVecInfo[i]->outRPLExt = CopyString(&gcheap, buf);
                }
                /* out dir */
                sprintf(cmd, "REPLACEABLENVECOUTDIR%d", i);
                if (GetConfStr(cParm, nParm, cmd, buf)) {
                    rplNVecInfo[i]->outRPLDir = CopyString(&gcheap, buf);
                }
            }
        }
    }

    if (TRUE) {
            /* GPU/MKL/CPU */              /* discard: should be set when compiling */
            /* THREADS */
            /* SGD/HF */
            /* LEARNING RATE SCHEDULE */
            /*     RELATED STUFFS */
    }
}

/* cz277 - xform */
Boolean LoadInRplNMat(ANNSet *annSet, NMatrix *rplNMat) {
    int rplIdx = rplNMat->rplIdx;

    if (rplIdx < 1 || rplIdx >= MAXNUMRPLNMAT) {
        HError(9999, "LoadInRplNMat: Replaceable matrix index %d out of range", rplIdx);
    }
    if (rplNMatInfo[rplIdx] == NULL) {
        /*HError(9999, "LoadInRplNMat: XForm info for replaceable matrix %d unset", rplIdx);*/
        return TRUE;
    }
    if (annSet->rplNMatInfo[rplIdx] == NULL) {
        annSet->rplNMatInfo[rplIdx] = rplNMatInfo[rplIdx];
    }
    else if (annSet->rplNMatInfo[rplIdx] != rplNMatInfo[rplIdx]) {
        HError(9999, "LoadInRplNMat: A different replaceable part info was loaded");
    }
    if (annSet->rplNMatInfo[rplIdx]->rplNMat == NULL) {
        annSet->rplNMatInfo[rplIdx]->rplNMat = rplNMat;
        annSet->rplNMatInfo[rplIdx]->rplNMat->rplIdx = rplIdx;
        /*annSet->rplNMatInfo[rplIdx]->saveRplNMat = CloneNMatrix(&gcheap, &rplNMat[rplIdx]);*/
        annSet->rplNMatInfo[rplIdx]->saveRplNMatHost.rowNum = rplNMat->rowNum;
        annSet->rplNMatInfo[rplIdx]->saveRplNMatHost.colNum = rplNMat->colNum;
        annSet->rplNMatInfo[rplIdx]->saveRplNMatHost.matElems = rplNMat->matElems;
        annSet->rplNMatInfo[rplIdx]->inActive = TRUE;
    }
    else if (annSet->rplNMatInfo[rplIdx]->rplNMat != rplNMat) {
        HError(9999, "LoadInRplNMat: A different NMatrix was set");
	/*annSet->rplNMatInfo[rplIdx]->rplNMat = rplNMat;*/
    }

    return TRUE;
}

/* cz277 - xform */ 
Boolean LoadInRplNVec(ANNSet *annSet, NVector *rplNVec) {
    int rplIdx = rplNVec->rplIdx;

    if (rplIdx < 1 || rplIdx >= MAXNUMRPLNVEC) {
        HError(9999, "LoadInRplNVec: Replaceable vector index %d out of range", rplIdx);
    }
    if (rplNVecInfo[rplIdx] == NULL) {
        /*HError(9999, "LoadInRplNVec: Mask for replaceable vector %d unset", rplIdx);*/
        return TRUE;
    }
    if (annSet->rplNVecInfo[rplIdx] == NULL) {
        annSet->rplNVecInfo[rplIdx] = rplNVecInfo[rplIdx];
    }
    else if (annSet->rplNVecInfo[rplIdx] != rplNVecInfo[rplIdx]) {
        HError(9999, "LoadInRplNVec: A different replaceable part info was loaded");
    }
    if (annSet->rplNVecInfo[rplIdx]->rplNVec == NULL) {
        annSet->rplNVecInfo[rplIdx]->rplNVec = rplNVec;
        annSet->rplNVecInfo[rplIdx]->rplNVec->rplIdx = rplIdx;
        /*annSet->rplNVecInfo[rplIdx]->saveRplNVec = CloneNVector(&gcheap, &rplNVec[rplIdx]);*/
        annSet->rplNVecInfo[rplIdx]->saveRplNVecHost.vecLen = rplNVec->vecLen;
        annSet->rplNVecInfo[rplIdx]->saveRplNVecHost.vecElems = rplNVec->vecElems;
        annSet->rplNVecInfo[rplIdx]->inActive = TRUE;
    }
    else if (annSet->rplNVecInfo[rplIdx]->rplNVec != rplNVec) {
        HError(9999, "LoadInRplNVec: A different NVector was set");
	/*annSet->rplNVecInfo[rplIdx]->rplNVec = rplNVec;*/
    }

    return TRUE;
}

/* set the update flag for each ANN layer */
void SetUpdateFlags(ANNSet *annSet) {
    int i;
    AILink curAI;
    ADLink annDef;
    LELink layerElem;
    char *charPtr = NULL;
    char buf[256];
    
    if ((trace & T_TOP) && (hasShownUpdtFlag == FALSE)) {
        printf("SetUpdateFlags: Updating ");
    }

    if (updtFlagStr != NULL) {
        strcpy(buf, updtFlagStr);
        charPtr = strtok(buf, ",");
        /*charPtr = strtok(updtFlagStr, ",");*/
    }
    curAI = annSet->defsHead;
    while (curAI != NULL) {
        annDef = curAI->annDef;
        for (i = 0; i < annDef->layerNum; ++i) {
            layerElem = annDef->layerList[i];
            if (charPtr != NULL) {
                layerElem->trainInfo->updtFlag = atoi(charPtr);
                charPtr = strtok(NULL, ",");
            }
            else {
                layerElem->trainInfo->updtFlag = ACTFUNUK | BIASUK | WEIGHTUK;
            }
            if ((trace & T_TOP) && (hasShownUpdtFlag == FALSE)) {
                if (!(layerElem->trainInfo->updtFlag & (ACTFUNUK | BIASUK | WEIGHTUK | INITUK))) {
                    printf(", NoParam");
                }
                else {
                    printf(", ");
                    if (layerElem->trainInfo->updtFlag & ACTFUNUK) { 
                        printf("+ActFun");
                    }
                    if (layerElem->trainInfo->updtFlag & BIASUK) { 
                        printf("+Bias");
                    }
                    if (layerElem->trainInfo->updtFlag & WEIGHTUK) {
                        printf("+Weight");
                    }
                    /* cz277 - pact */
                    if (layerElem->trainInfo->updtFlag & INITUK) {
                        printf("+ActFun(Init)");
                    }
                }
            }
        }
        curAI = curAI->next;
    }

    if ((trace & T_TOP) && (hasShownUpdtFlag == FALSE)) {
        printf("\n");
        hasShownUpdtFlag = TRUE;
    }
}

static inline void FillBatchFromFeaMixOLD(FeaMix *feaMix, int batLen, int *CMDVecPL) {
    int i, j, k, srcOff = 0, curOff = 0, dstOff, hisOff, hisDim;
    FELink feaElem;

    /* if it is the shared */
    if (feaMix->feaList[0]->feaMats[0] == feaMix->mixMats[0]) {
        return;
    }
    /* cz277 - 1007 */
    if (feaMix->batIdx > batIdx + 1 || feaMix->batIdx < batIdx) {
        HError(9999, "FillBatchFromFeaMix: batIdx of this feature mix does not match the global index");
    }
    else if (feaMix->batIdx == batIdx + 1) {
        return;
    }
    else {
        ++feaMix->batIdx;
    }

    /* otherwise, fill the batch with a mixture of the FeaElem */
    for (i = 0; i < feaMix->elemNum; ++i) {
        feaElem = feaMix->feaList[i];

        if (feaElem->inputKind == INPFEAIK || feaElem->inputKind == AUGFEAIK) {
            for (j = 0, srcOff = 0, dstOff = curOff; j < batLen; ++j, srcOff += feaElem->extDim, dstOff += feaMix->mixDim) {
                CopyNSegment(feaElem->feaMats[0], srcOff, feaElem->extDim, feaMix->mixMats[0], dstOff);
            }
        }
        else if (feaElem->inputKind == ANNFEAIK) {  /* ANNFEAIK, left context is consecutive */
            for (j = 0; j < batLen; ++j) {
                /* cz277 - gap */
                hisDim = feaElem->hisLen * feaElem->feaDim;
                hisOff = j * hisDim;
                if (CMDVecPL != NULL && feaElem->hisMat != NULL) {
                    if (CMDVecPL[j] == 0) {	/* reset the history */
			ClearNMatrixSegment(feaElem->hisMat, hisOff, hisDim);
                    }
                    else if (CMDVecPL[j] > 0) {	/* shift the history */
                        CopyNSegment(feaElem->hisMat, CMDVecPL[j] * hisDim, hisDim, feaElem->hisMat, hisOff);
                    }
                }
                /* standard operations */
                dstOff = j * feaMix->mixDim + curOff;
                for (k = 1; k <= feaElem->ctxMap[0]; ++k, dstOff += feaElem->feaDim) { 
                    if (feaElem->ctxMap[k] < 0) {
                        /* first, previous segments from hisMat to feaMix->mixMat */
                        srcOff = ((j + 1) * feaElem->hisLen + feaElem->ctxMap[k]) * feaElem->feaDim;
                        CopyNSegment(feaElem->hisMat, srcOff, feaElem->feaDim, feaMix->mixMats[0], dstOff);
                    }
                    else if (feaElem->ctxMap[k] == 0) {
                        /* second, copy current segment from feaMat to feaMix->mixMat */
                        srcOff = j * feaElem->srcDim + feaElem->dimOff;
                        CopyNSegment(feaElem->feaMats[0], srcOff, feaElem->feaDim, feaMix->mixMats[0], dstOff);
                    }
                    else {
                        HError(9999, "FillBatchFromFeaMix: The future of ANN features are not applicable");
                    }
                }
                /* shift history info in hisMat and copy current segment from feaMat to hisMat */
                if (feaElem->hisMat != NULL) {
                    dstOff = hisOff;
                    srcOff = dstOff + feaElem->feaDim;
                    for (k = 0; k < feaElem->hisLen - 1; ++k, srcOff += feaElem->feaDim, dstOff += feaElem->feaDim) {
                        CopyNSegment(feaElem->hisMat, srcOff, feaElem->feaDim, feaElem->hisMat, dstOff);    
                    }
                    srcOff = j * feaElem->srcDim + feaElem->dimOff;
                    CopyNSegment(feaElem->feaMats[0], srcOff, feaElem->feaDim, feaElem->hisMat, dstOff);
                }
            }
        }
        curOff += feaElem->extDim;
    }
}

/* cz277 - xform */
static inline void FillBatchFromFeaMix(LELink layerElem, int batLen) {
    int i, j, k, l, n, m, srcOff, curOff, dstOff, t, c, curCtx, pos;
    FELink feaElem;
    FeaMix *feaMix;
    NMatrix *mixMat, *feaMat;

    feaMix = layerElem->feaMix;
    /* if mixMats are shared with feaMats -- no need to reload the features */
    if (feaMix->elemNum == 1) {
        feaElem = feaMix->feaList[0];
        if (!(feaElem->inputKind == ANNFEAIK && IntVecSize(feaElem->ctxMap) > 1))
            return;
    }
    /* if feaMat is shared and is processed for current batch */
    if (feaMix->batIdx > batIdx + 1 || feaMix->batIdx < batIdx)
        HError(9999, "FillBatchFromFeaMix: batIdx of this feature mix does not match the global index");
    else if (feaMix->batIdx == batIdx + 1)
        return;
    else
        ++feaMix->batIdx;
    /* otherwise */
    n = IntVecSize(layerElem->drvCtx);
    for (i = 1, j = 1; i <= n; ++i) {
        while (layerElem->drvCtx[i] != feaMix->ctxPool[j])
            ++j;
        mixMat = feaMix->mixMats[j];
        for (k = 0, curOff = 0; k < feaMix->elemNum; ++k) {
            feaElem = feaMix->feaList[k];
            l = 1;
            /* 1. extended input features */
            if (feaElem->inputKind == INPFEAIK || feaElem->inputKind == AUGFEAIK) {
                if (feaElem->inputKind == INPFEAIK) {
                    while (layerElem->drvCtx[i] != feaElem->ctxPool[l])
                        ++l;
                }
                feaMat = feaElem->feaMats[l];
                for (t = 0, srcOff = 0, dstOff = curOff; t < batLen; ++t, srcOff += feaElem->extDim, dstOff += feaMix->mixDim) {
                    CopyNSegment(feaMat, srcOff, feaElem->extDim, mixMat, dstOff);
                }
            }
            else if (feaElem->inputKind == ANNFEAIK) {	/* 2. ANN features */
                m = IntVecSize(feaElem->ctxMap);
                for (c = 1; c <= m; ++c) {
                    curCtx = feaElem->ctxMap[c] + layerElem->drvCtx[i];
                    while (curCtx != feaElem->ctxPool[l])
                        ++l;
                    feaMat = feaElem->feaMats[l];
                    for (t = 0; t < batLen; ++t) {
                        srcOff = t * feaElem->srcDim + feaElem->dimOff;
                        dstOff = t * feaMix->mixDim + curOff + (c - 1) * feaElem->feaDim;
                        CopyNSegment(feaMat, srcOff, feaElem->feaDim, mixMat, dstOff);
                    }
                }
            }
            curOff += feaElem->extDim;
        }
    }
}


/* fill a batch with error signal */
static inline void FillBatchFromErrMixOLD(FeaMix *errMix, int batLen, NMatrix *mixMat) {
    int i, j, srcOff, dstOff;
    FELink errElem;

    /* if it is the shared */
    if (errMix->feaList[0]->feaMats[1] == mixMat) {
        return;
    }

    /* otherwise, fill the batch with a mixture of the FeaElem */
    dstOff = 0;
    /* reset mixMat to 0 */
    /*SetNMatrix(0.0, mixMat, batLen);*/
    ClearNMatrix(mixMat, batLen);
    /* accumulate the error signals from each source */
    for (i = 0; i < batLen; ++i) {
        for (j = 0; j < errMix->elemNum; ++j) {
            errElem = errMix->feaList[j];
            srcOff = i * errElem->srcDim + errElem->dimOff;
            AddNSegment(errElem->feaMats[1], srcOff, errElem->extDim, mixMat, dstOff);
            dstOff += errElem->extDim;
        }
    }
}

/* cz277 - many */
static inline void FillBatchFromErrMix(LELink layerElem, int batLen) {
    int c, i, j, k, l, m, n, t, srcOff, dstOff;
    FELink errElem;
    FeaMix *errMix;

    if (layerElem->errMix == NULL)
        return;
    errMix = layerElem->errMix;
    if (errMix->elemNum == 1) {
        errElem = errMix->feaList[0];
        if (IntVecSize(errElem->ctxMap) == 1)
            if (errElem->srcDim == errElem->feaDim)
                return;
    }
    
    n = IntVecSize(layerElem->drvCtx);
    for (i = 1; i <= n; ++i) {
        ClearNMatrix(errMix->mixMats[i], batLen);
    }
    for (i = 0; i < errMix->elemNum; ++i) {
        errElem = errMix->feaList[i];
        m = IntVecSize(errElem->ctxPool);
        n = IntVecSize(errElem->ctxMap);
        for (j = 1; j <= m; ++j) {
            for (k = 1; k <= n; ++k) {
                srcOff = errElem->dimOff + (k - 1) * errElem->feaDim;
                c = errElem->ctxPool[j] + errElem->ctxMap[k];
                l = 1;
                while (errMix->ctxPool[l] != c)
                    ++l;
                for (t = 0, dstOff = 0; t < batLen; ++t, srcOff += errElem->srcDim, dstOff += errElem->feaDim) {
                    AddNSegment(errElem->feaMats[j], srcOff, errElem->feaDim, errMix->mixMats[l], dstOff);
                }
            }
        }
    }
    /* scale the mixMats */
    /*n = IntVecSize(layerElem->drvCtx);
    for (i = 1; i <= n; ++i) {
        if (layerElem->trainInfo->drvCnt[i] > 1)
            ScaleNMatrix(1.0 / (float) layerElem->trainInfo->drvCnt[i], batLen, errMix->mixDim, errMix->mixMats[i]);
    }*/

}


/* temp function */
void ShowAddress(ANNSet *annSet) {
    int i;
    AILink curAI;
    ADLink annDef;
    LELink layerElem;

    curAI = annSet->defsHead;
    while (curAI != NULL) {
        annDef = curAI->annDef;
        printf("ANNInfo = %p. ANNDef = %p: \n", curAI, annDef);
        for (i = 0; i < annDef->layerNum; ++i) {
            layerElem = annDef->layerList[i];
            /*printf("layerElem = %p, feaMix[0]->feaMat = %p, xFeaMat = %p, yFeaMat = %p, trainInfo = %p, dxFeaMat = %p, dyFeaMat = %p, labMat = %p\n", layerElem, layerElem->feaMix->feaList[0]->feaMat, layerElem->xFeaMat, layerElem->yFeaMat, layerElem->trainInfo, layerElem->trainInfo->dxFeaMat, layerElem->trainInfo->dyFeaMat, layerElem->trainInfo->labMat);*/
        }
        printf("\n");
        curAI = curAI->next;
    }
}

/* update the map sum matrix for outputs */
void UpdateOutMatMapSum(ANNSet *annSet, int batLen, int streamIdx) {

    /* cz277 - many */
    HNBlasTNgemm(annSet->mapStruct->mappedTargetNum, batLen, annSet->outLayers[streamIdx]->nodeNum, 1.0, annSet->mapStruct->maskMatMapSum[streamIdx], annSet->outLayers[streamIdx]->yFeaMats[1], 0.0, annSet->mapStruct->outMatMapSum[streamIdx]);
}

/* update the map sum matrix for labels */
void UpdateLabMatMapSum(ANNSet *annSet, int batLen, int streamIdx) {

    HNBlasTNgemm(annSet->mapStruct->mappedTargetNum, batLen, annSet->outLayers[streamIdx]->nodeNum, 1.0, annSet->mapStruct->maskMatMapSum[streamIdx], annSet->outLayers[streamIdx]->trainInfo->labMat, 0.0, annSet->mapStruct->labMatMapSum[streamIdx]);
}

/* cz277 - pact */
/* y = 1 / sqrt(var) * x + (- mean / sqrt(var)) */
static void InitAffineScaleByVar(int vecLen, NVector *varVec) {
    int i;
  
    if (!(vecLen <= varVec)) {
        HError(9999, "InitAffineScaleByVar: Wrong vector length");
    }
#ifdef CUDA
    SyncNVectorDev2Host(varVec);
#endif
    /* convert variance */
    for (i = 0; i < vecLen; ++i) {
        if (varVec->vecElems[i] <= 0.0) {
            HError(9999, "InitAffineScaleByVar: variance should be > 0.0");
        }
        varVec->vecElems[i] = 1.0 / sqrt(varVec->vecElems[i]);
    }
#ifdef CUDA
    SyncNVectorHost2Dev(varVec);
#endif
}

/* cz277 - pact */
/* y = 1 / sqrt(var) * x + (- mean / sqrt(var)) */
static void InitAffineShiftByMean(int vecLen, NVector *scaleVec, NVector *meanVec) {
    int i;
    
    if (!(vecLen <= meanVec->vecLen && vecLen <= scaleVec)) {
        HError(9999, "InitAffineShiftByMean: Wrong vector length");
    }
#ifdef CUDA
    SyncNVectorDev2Host(meanVec);
    SyncNVectorDev2Host(scaleVec);
#endif
    /* convert mean */
    for (i = 0; i < vecLen; ++i) {
        meanVec[i] = (-1.0) * meanVec[i] * scaleVec->vecElems[i];
    }
#ifdef CUDA
    SyncNVectorHost2Dev(meanVec);
    SyncNVectorHost2Dev(scaleVec);
#endif
}

/* cz277 - pact */
void DoStaticUpdateOperation(int status, int drvIdx, LELink layerElem, int batLen) {
    double nSamples;
    size_t *cnt1, *cnt2;

    if (layerElem->trainInfo == NULL)
        return;
    if ((layerElem->trainInfo->updtFlag & INITUK) == 0)
        return;

    switch (layerElem->actFunInfo.actFunKind) {
    case AFFINEAF:
        if (layerElem->drvCtx[drvIdx] == 0) {
            cnt1 = (size_t *) layerElem->actFunInfo.actParmVec[1]->hook;
            cnt2 = (size_t *) layerElem->actFunInfo.actParmVec[2]->hook;
            switch (status) {
	    case 0:
	        *cnt1 += batLen;
                *cnt2 += batLen;	
		break;
            case 1:
		nSamples = *cnt1;
                if (nSamples == 0)
                    HError(-9999, "DoStaticUpdateOperation: nSamples = 0, inf will generate");
                AccMeanNVector(layerElem->yFeaMats[drvIdx], batLen, layerElem->nodeNum, (NFloat) nSamples, layerElem->actFunInfo.actParmVec[1]);
                break;
            case 2:
                nSamples = *cnt2;
                if (nSamples == 0)
                    HError(-9999, "DoStaticUpdateOperation: nSamples = 0, inf will generate");
                AccVarianceNVector(layerElem->yFeaMats[drvIdx], batLen, layerElem->nodeNum, (NFloat) nSamples, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2]);
                break;
            case 3:
                nSamples = *cnt2;
                if (nSamples > 0) {
                    InitAffineScaleByVar(layerElem->nodeNum, layerElem->actFunInfo.actParmVec[2]);
                    *cnt2 = 0;
                }
                break;
            case 4:
                nSamples = *cnt1;
                if (nSamples > 0) {
                    InitAffineShiftByMean(layerElem->nodeNum, layerElem->actFunInfo.actParmVec[2], layerElem->actFunInfo.actParmVec[1]);
                    *cnt1 = 0;
                }
                break;
            default:
                break;
            }
        }
        break;
    default:
        break;
    }
}

/* cz277 - pact */
void ForwardPropBlank(ANNSet *annSet, int batLen) {
    int i, j, n;
    AILink curAI;
    ADLink annDef;
    LELink layerElem;

    /* init the ANNInfo pointer */
    curAI = annSet->defsHead;
    /* proceed in the forward fashion */
    while (curAI != NULL) {
        /* fetch current ANNDef */
        annDef = curAI->annDef;
        /* proceed layer by layer */
        for (i = 0; i < annDef->layerNum; ++i) {
            /* get current LayerElem */
            layerElem = annDef->layerList[i];
            n = IntVecSize(layerElem->drvCtx);
            for (j = 1; j <= n; ++j) {
                DoStaticUpdateOperation(layerElem->status, j, layerElem, batLen);
            }
        }
        curAI = curAI->next;
    }
}

/* the batch with input features are assumed to be filled */
void ForwardPropBatch(ANNSet *annSet, int batLen, int *CMDVecPL) {
    int i, j, n;
    AILink curAI;
    ADLink annDef;
    LELink layerElem;

    /* init the ANNInfo pointer */
    curAI = annSet->defsHead;
    /* proceed in the forward fashion */
    while (curAI != NULL) {
        /* fetch current ANNDef */
        annDef = curAI->annDef;
        /* proceed layer by layer */
        for (i = 0; i < annDef->layerNum; ++i) {
            /* get current LayerElem */
            layerElem = annDef->layerList[i];
            n = IntVecSize(layerElem->drvCtx);
            FillBatchFromFeaMix(layerElem, batLen);
            for (j = 1; j <= n; ++j) {
/*chaopig*/
/*printf("\n\nINPUT\n\n");
ShowNMatrix(layerElem->xFeaMats[j], 1);*/
                /* at least the batch (feaMat) for each FeaElem is already */
                /*FillBatchFromFeaMix(layerElem, batLen, j);*/
                /* do the operation of current layer */
                switch (layerElem->operKind) {
                case MAXOK:

                    break;
                case SUMOK: 
                    /* y = b, B^T should be row major matrix, duplicate the bias vectors */ 
                    DupNVector(layerElem->biasVec, layerElem->yFeaMats[j], batLen);
                    /* y += w * b, X^T is row major, W^T is column major, Y^T = X^T * W^T + B^T */
                    HNBlasTNgemm(layerElem->nodeNum, batLen, layerElem->inputDim, 1.0, layerElem->wghtMat, layerElem->xFeaMats[j], 1.0, layerElem->yFeaMats[j]);
                    break;
                case PRODOK:

                    break;
                default:
                    HError(9999, "ForwardPropBatch: Unknown layer operation kind");
                }
                /* cz277 - pact */
                DoStaticUpdateOperation(layerElem->status, j, layerElem, batLen);

                /* apply activation transformation */
                switch (layerElem->actFunInfo.actFunKind) {
                case AFFINEAF:
/*chaopig*/
/*printf("\n\ncmllr weight\n");
ShowNMatrix(layerElem->wghtMat, 1);
printf("\n\ncmllr bias\n");
ShowNVector(layerElem->biasVec);
printf("\n\nBEFORE\n");
ShowNMatrix(layerElem->yFeaMats[j], 1);*/

                    ApplyAffineAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]);	/* cz277 - pact */
/*chaopig*/
/*printf("\n\nAFTER\n");
ShowNMatrix(layerElem->yFeaMats[j], 1);
exit(1);*/
                    break;
                case HERMITEAF:
                    ApplyHermiteAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                    break;
                case LEAKYRELUAF:
                    ApplyReLUAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, RELUNEGSCALE, layerElem->yFeaMats[j]);
                    break;
                case LHUCSIGMOIDAF:
                    ApplyLHUCSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                    break;
                case LINEARAF:
                    break;
                case PASIGMOIDAF:
                    ApplyPASigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                    break; 
                case PABSIGMOIDAF:
                    if (layerElem->trainInfo != NULL) {
                        CopyNSegment(layerElem->yFeaMats[j], 0, batLen * layerElem->nodeNum, layerElem->trainInfo->gradInfo->actCacheMats[j], 0);
                    }
                    ApplyPABSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]);
                    break;
                case PBSIGMOIDAF:
                    if (layerElem->trainInfo != NULL) {
                        CopyNSegment(layerElem->yFeaMats[j], 0, batLen * layerElem->nodeNum, layerElem->trainInfo->gradInfo->actCacheMats[j], 0);
                    }
/*chaopig*/
/*printf("\n\nACTPARM:\n");
ShowNVector(layerElem->actFunInfo.actParmVec1);
printf("\n\nBEFORE:\n");
ShowNMatrix(layerElem->yFeaMats[j], 1);*/
                    ApplyPBSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
/*chaopig*/
/*printf("\n\nAFTER:\n");
ShowNMatrix(layerElem->yFeaMats[j], 1);
exit(1);*/
                    break;
                case PABCSIGMOIDAF:
                    if (layerElem->trainInfo != NULL) {
                        CopyNSegment(layerElem->yFeaMats[j], 0, batLen * layerElem->nodeNum, layerElem->trainInfo->gradInfo->actCacheMats[j], 0);
                    }
                    ApplyPABCSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->actFunInfo.actParmVec[3], layerElem->yFeaMats[j]);
                    break;
	        case PLRELUAF:
                    if (layerElem->trainInfo != NULL) {
                        CopyNSegment(layerElem->yFeaMats[j], 0, batLen * layerElem->nodeNum, layerElem->trainInfo->gradInfo->actCacheMats[j], 0);
                    }
                    ApplyPLReLUAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                    break;
                case POLYLINEARAF:
                    if (layerElem->trainInfo != NULL) {
                        CopyNSegment(layerElem->yFeaMats[j], 0, batLen * layerElem->nodeNum, layerElem->trainInfo->gradInfo->actCacheMats[j], 0);
                    }
                    ApplyPolyLinearAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]);
                    break;
                case PRELUAF:
                    if (layerElem->trainInfo != NULL) {
                        CopyNSegment(layerElem->yFeaMats[j], 0, batLen * layerElem->nodeNum, layerElem->trainInfo->gradInfo->actCacheMats[j], 0);
                    }
/*chaopig*/
/*printf("\n\nA:\n");
ShowNMatrix(layerElem->yFeaMats[j], 1);*/
                    ApplyPReLUAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
/*chaopig*/
/*printf("\n\nB:\n");
ShowNMatrix(layerElem->yFeaMats[j], 1);
printf("\n\nC actParmVec1:\n");
ShowNVector(layerElem->actFunInfo.actParmVec1);
exit(1);*/
                    break;
                case RELUAF:
                    ApplyReLUAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, 0.0, layerElem->yFeaMats[j]);
                    break;
                case SIGMOIDAF:
                    ApplySigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                    break;
                case SOFTMAXAF:
                    ApplySoftmaxAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                    break;
                case SOFTRELUAF:
                    ApplySoftReLAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                    break;
                case SOFTSIGNAF:
                    ApplySoftSignAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                    break;
                case TANHAF:
                    ApplyTanHAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                    break;
                default:
                    HError(9999, "ForwardPropBatch: Unknown activation function kind");
                }
            }
        }

        /* get the next ANNDef */
        curAI = curAI->next;
    }

    SetBatchIndex(GetBatchIndex() + 1);

}

/* function to compute the error signal for frame level criteria (for sequence level, do nothing) */
void CalcOutLayerBackwardSignal(LELink layerElem, int batLen, ObjFunKind objfunKind, int ctxIdx) {

    if (layerElem->roleKind != OUTRK) {
        HError(9999, "CalcOutLayerBackwardSignal: Function only valid for output layers");
    }
    if (ctxIdx != 1 || layerElem->drvCtx[ctxIdx] != 0) {
        HError(9999, "CalcOutLayerBackwardSignal: Out layer only support single current frame at the moment");
    }
    switch (objfunKind) {
    case MMSEOF:
        /* proceed for MMSE objective function */
        switch (layerElem->actFunInfo.actFunKind) {
        case HERMITEAF:
            break;
        case LINEARAF:
            break;
        case RELUAF:
            break;
        case SIGMOIDAF:
            break;
        case SOFTMAXAF:
            break;
        case SOFTRELUAF:
            break;
        case SOFTSIGNAF:
            break;
        case TANHAF:
            break;
        default:
            HError(9999, "CalcOutLayerBackwardSignal: Unknown output activation function");
        }
        break;
    case XENTOF:
        /* proceed for XENT objective function */
        switch (layerElem->actFunInfo.actFunKind) {
        case HERMITEAF:
            break;
        case LINEARAF:
            break;
        case RELUAF:
            break;
        case SIGMOIDAF:
            break;
        case SOFTMAXAF:
            SubNMatrix(layerElem->yFeaMats[ctxIdx], layerElem->trainInfo->labMat, batLen, layerElem->nodeNum, layerElem->yFeaMats[ctxIdx]);
            break;
        case SOFTRELUAF:
            break;
        case SOFTSIGNAF:
            break;
        case TANHAF:
            break;
        default:
            HError(9999, "CalcOutLayerBackwardSignal: Unknown output activation function");
        }
        break;
    case MLOF:
    case MMIOF:
    case MPEOF:
    case MWEOF:
    case SMBROF:
        break;
    default:
        HError(9999, "CalcOutLayerBackwardSignal: Unknown objective function kind");
    }
}

/* cz277 - gradprobe */
#ifdef GRADPROBE
void AccGradProbeWeight(LayerElem *layerElem) {
    int i, j, k, size;
    NFloat *wghtMat;

#ifdef CUDA
    SyncNMatrixDev2Host(layerElem->trainInfo->gradInfo->wghtMat);
#endif
    wghtMat = layerElem->trainInfo->gradInfo->wghtMat->matElems;
    /* weights */
    size = layerElem->nodeNum * layerElem->inputDim;
    j = DVectorSize(layerElem->wghtGradInfoVec);
    for (i = 0; i < size; ++i) {
        if (wghtMat[i] > layerElem->maxWghtGrad) 
            layerElem->maxWghtGrad = wghtMat[i];
        if (wghtMat[i] < layerElem->minWghtGrad)
            layerElem->minWghtGrad = wghtMat[i];
        layerElem->meanWghtGrad += wghtMat[i];
        k = wghtMat[i] / PROBERESOLUTE + j / 2;
        layerElem->wghtGradInfoVec[k + 1] += 1;
    }
}
#endif

/* cz277 - gradprobe */
#ifdef GRADPROBE
void AccGradProbeBias(LayerElem *layerElem) {
    int i, j, k, size;
    NFloat *biasVec;

#ifdef CUDA
    SyncNVectorDev2Host(layerElem->trainInfo->gradInfo->biasVec);
#endif
    biasVec = layerElem->trainInfo->gradInfo->biasVec->vecElems;
    /* biases */
    size = layerElem->nodeNum;
    j = DVectorSize(layerElem->biasGradInfoVec);
    for (i = 0; i < size; ++i) {
        if (biasVec[i] > layerElem->maxBiasGrad)
            layerElem->maxBiasGrad = biasVec[i];
        if (biasVec[i] < layerElem->minBiasGrad)
            layerElem->minBiasGrad = biasVec[i];
        layerElem->meanBiasGrad += biasVec[i];
        k = biasVec[i] / PROBERESOLUTE + j / 2;
        layerElem->biasGradInfoVec[k + 1] += 1;
    }
}
#endif

/* delta_j = h'(a_j) * sum_k [w_k,j * delta_k] */
/*   dtl_j = sum_k [w_k,j * dtl_k * h'(a_k)] */
/*   dtl_j = delta_j / h'(a_j) */
/* backward propagation algorithm */
void BackwardPropBatch(ANNSet *annSet, int batLen, Boolean accGrad) {
    int i, j, n;
    AILink curAI;
    ADLink annDef;
    LELink layerElem;
    NMatrix *dyFeaMat;
    Boolean accFlag;

    /* init the ANNInfo pointer */
    curAI = annSet->defsTail;
    /* proceed in the backward fashion */
    while (curAI != NULL) {
        /* fetch current ANNDef */
        annDef = curAI->annDef;
        /* proceed layer by layer */
        for (i = annDef->layerNum - 1; i >= 0; --i) {
            /* get current LayerElem */
            layerElem = annDef->layerList[i];
            n = IntVecSize(layerElem->drvCtx);
            FillBatchFromErrMix(layerElem, batLen);	/* cz277 - many */
            for (j = 1; j <= n; ++j) {
                if (j == 1) 
                    accFlag = accGrad;
                else
                    accFlag = TRUE;
                /* proceed different types of layers */
                if (layerElem->roleKind == OUTRK) {
                    /* set dyFeaMat */
                    CalcOutLayerBackwardSignal(layerElem, batLen, annDef->objfunKind, j);
                    dyFeaMat = layerElem->yFeaMats[j];	/* delta_k */
                }
                else {
                    /* at least the batch (feaMat) for each FeaElem is already */
                    /*FillBatchFromErrMix(layerElem, batLen, j);*/	/* cz277 - many */
                    dyFeaMat = layerElem->trainInfo->dyFeaMats[j];	/* sum_k w_{k,j} * delta_k */
                    /* cz277 - pact, compute the graident for the trainable hidden activation function gradients here */
                    switch (layerElem->actFunInfo.actFunKind) {
                    case AFFINEAF:
                        ApplyTrAffineAct(dyFeaMat, layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], accFlag, layerElem->trainInfo->gradInfo->actParmVec[1], layerElem->trainInfo->gradInfo->actParmVec[2]);
                        break;
                    case HERMITEAF:

                        break;
                    case LHUCSIGMOIDAF:
                        ApplyTrLHUCSigmoidAct(dyFeaMat, layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], accFlag, layerElem->trainInfo->gradInfo->actParmVec[1]);
                        break;
                    case PASIGMOIDAF:
                        ApplyTrPASigmoidAct(dyFeaMat, layerElem->yFeaMats[j], layerElem->actFunInfo.actParmVec[1], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1]);
                        break;
                    case PABSIGMOIDAF:
                        ApplyTrPABSigmoidAct(dyFeaMat, layerElem->trainInfo->gradInfo->actCacheMats[j], layerElem->yFeaMats[j], layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1], layerElem->trainInfo->gradInfo->actParmVec[2]);
                        break;
                    case PBSIGMOIDAF:
                        ApplyTrPBSigmoidAct(dyFeaMat, layerElem->trainInfo->gradInfo->actCacheMats[j], layerElem->yFeaMats[j], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1]); 
                        break;
                    case PABCSIGMOIDAF:
                        ApplyTrPABCSigmoidAct(dyFeaMat, layerElem->trainInfo->gradInfo->actCacheMats[j], layerElem->yFeaMats[j], layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->actFunInfo.actParmVec[3], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1], layerElem->trainInfo->gradInfo->actParmVec[2], layerElem->trainInfo->gradInfo->actParmVec[3]);
                        break;
                    case PLRELUAF:
/*chaopig*/
/*printf("BEFORE TrGradients:\n");
ShowNVector(layerElem->trainInfo->gradInfo->actParmVec1);*/
                        ApplyTrPLReLUAct(dyFeaMat, layerElem->trainInfo->gradInfo->actCacheMats[j], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1]);
/*chaopig*/
/*printf("AFTER TrGradients: accFlags = %d\n", accFlag);
ShowNVector(layerElem->trainInfo->gradInfo->actParmVec1);
exit(1);*/
                        break;
                    case POLYLINEARAF:
                        ApplyTrPolyLinearAct(dyFeaMat, layerElem->trainInfo->gradInfo->actCacheMats[j], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1], layerElem->trainInfo->gradInfo->actParmVec[2]);
                        break;
                    case PRELUAF:
                        ApplyTrPReLUAct(dyFeaMat, layerElem->trainInfo->gradInfo->actCacheMats[j], batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->actParmVec[1]);
                        break;
                    default:
                        break;
                    }
                    /* apply activation transformation */
                    switch (layerElem->actFunInfo.actFunKind) {
                    case AFFINEAF:
                        ApplyDAffineAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]);	/* h(a_j) -> h'(a_j) */
                        break;
                    case HERMITEAF:
                    
                        break;
                    case LEAKYRELUAF:
                        ApplyDReLUAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, RELUNEGSCALE, layerElem->yFeaMats[j]);
                        break;
                    case LINEARAF:
                        ApplyDLinearAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                        break;
                    case LHUCSIGMOIDAF:
                        ApplyDLHUCSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                        break;
                    case PASIGMOIDAF:
                        ApplyDPASigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                        break;
                    case PABSIGMOIDAF:
                        ApplyDPABSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]); 
                        break;
                    case PBSIGMOIDAF:
                        ApplyDPBSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]); 
                        break;
                    case PABCSIGMOIDAF:
                        ApplyDPABCSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]);
                        break;
                    case PLRELUAF:
                        ApplyDPLReLUAct(layerElem->trainInfo->gradInfo->actCacheMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                        break;
                    case POLYLINEARAF:
                        ApplyDPolyLinearAct(layerElem->trainInfo->gradInfo->actCacheMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->actFunInfo.actParmVec[2], layerElem->yFeaMats[j]);
                        break;
                    case PRELUAF:
                        ApplyDPReLUAct(layerElem->trainInfo->gradInfo->actCacheMats[j], batLen, layerElem->nodeNum, layerElem->actFunInfo.actParmVec[1], layerElem->yFeaMats[j]);
                        break;
                    case RELUAF:
			ApplyDReLUAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, 0.0, layerElem->yFeaMats[j]);
                        break;
                    case SIGMOIDAF:
                        ApplyDSigmoidAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                        break;
                    case SOFTMAXAF:

                        break;
                    case SOFTRELUAF:
                        ApplyDSoftReLAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                        break;
                    case SOFTSIGNAF:

                        break;
                    case TANHAF:
                        ApplyDTanHAct(layerElem->yFeaMats[j], batLen, layerElem->nodeNum, layerElem->yFeaMats[j]);
                        break;
                    default:
                        HError(9999, "BackwardPropBatch: Unknown hidden activation function kind");
                    }
                    /* times sigma_k (dyFeaMat, from the next layer) */
                    /* dyFeaMat: sum_k w_{k,j} * delta_k -> delta_j */
                    MulNMatrix(layerElem->yFeaMats[j], dyFeaMat, batLen, layerElem->nodeNum, dyFeaMat);	/* delta_j = h'(a_j) * (sum_k w_{k,j} * delta_k) */
                }
                /* do current layer operation */
                switch (layerElem->operKind) {
                case MAXOK:

                    break;
                case SUMOK:
                    /* Y^T is row major, W^T is column major, X^T = Y^T * W^T */
                    HNBlasNNgemm(layerElem->inputDim, batLen, layerElem->nodeNum, 1.0, layerElem->wghtMat, dyFeaMat, 0.0, layerElem->trainInfo->dxFeaMats[j]);	/* sum_k w_{k,j} * delta_k */
                    break;
                case PRODOK:

                    break;
                default:
                    HError(9999, "BackwardPropBatch: Unknown layer operation kind");
                }
                /* compute and accumulate the updates */
                /* {layerElem->xFeaMat[n_frames * inputDim]}^T * dyFeaMat[n_frames * nodeNum] = deltaWeights[inputDim * nodeNum] */
                if (layerElem->trainInfo->updtFlag & WEIGHTUK) { 
                    HNBlasNTgemm(layerElem->inputDim, layerElem->nodeNum, batLen, 1.0, layerElem->xFeaMats[j], dyFeaMat, accFlag, layerElem->trainInfo->gradInfo->wghtMat);
                    /* cz277 - gradprobe */
#ifdef GRADPROBE
                    AccGradProbeWeight(layerElem);    
#endif
                }
                /* graidents for biases */
                if (layerElem->trainInfo->updtFlag & BIASUK) {
                    SumNMatrixByCol(dyFeaMat, batLen, layerElem->nodeNum, accFlag, layerElem->trainInfo->gradInfo->biasVec);
                    /* cz277 - gradprobe */
#ifdef GRADPROBE
                    AccGradProbeBias(layerElem);
#endif
                }

                if (layerElem->trainInfo->ssgInfo != NULL) {
                    /* attention: these two operations are gonna to change dyFeaMat elements to their square */
                    SquaredNMatrix(layerElem->xFeaMats[j], batLen, layerElem->inputDim, GetTmpNMat());
                    SquaredNMatrix(dyFeaMat, batLen, layerElem->nodeNum, dyFeaMat);
                    if (layerElem->trainInfo->updtFlag & WEIGHTUK)
                        HNBlasNTgemm(layerElem->inputDim, layerElem->nodeNum, batLen, 1.0, GetTmpNMat(), dyFeaMat, 1.0, layerElem->trainInfo->ssgInfo->wghtMat);
                    if (layerElem->trainInfo->updtFlag & BIASUK) 
                        SumNMatrixByCol(dyFeaMat, batLen, layerElem->nodeNum, TRUE, layerElem->trainInfo->ssgInfo->biasVec);
                }
            }
            /* scale the gradients */
            /*if (n > 1) {
                if (layerElem->trainInfo->updtFlag & WEIGHTUK) {
                    ScaleNMatrix(1.0 / (float) n, layerElem->inputDim, layerElem->nodeNum, layerElem->trainInfo->gradInfo->wghtMat); 
                }
                if (layerElem->trainInfo->updtFlag & BIASUK) {
                    ScaleNVector(1.0 / (float) n, layerElem->nodeNum, layerElem->trainInfo->gradInfo->biasVec);
                }
            }*/
        }

        /* get the previous ANNDef */
        curAI = curAI->prev;
    }

}

/* randomise an ANN layer */
void RandANNLayer(LELink layerElem, int seed, float scale) {
    float r;

    switch (layerElem->actFunInfo.actFunKind) {
    case LEAKYRELUAF:
    case LINEARAF:
    case PLRELUAF:
    case POLYLINEARAF:
    case PRELUAF:
    case RELUAF:
    case SOFTRELUAF:
        r = 16.0 / ((float) (layerElem->nodeNum + layerElem->inputDim));
	/* 0.004 for a (2000, 2000) layer; r = 0.001 for a (12000, 2000) layer */	
        r *= scale;
        RandInit(seed);
        RandNSegmentUniform(-1.0 * r, r, layerElem->wghtMat->rowNum * layerElem->wghtMat->colNum, layerElem->wghtMat->matElems);
        break;
        /*r = sqrt(2.0 / ((1.0 + pow(PLRELUNEGSCALE, 2.0)) * layerElem->nodeNum));     
        RandNSegmentGaussian(0.0, r, layerElem->wghtMat->rowNum * layerElem->wghtMat->colNum, layerElem->wghtMat->matElems);*/
    default:
        r = 4 * sqrt(6.0 / (float) (layerElem->nodeNum + layerElem->inputDim));
        r *= scale;
        RandInit(seed);
        RandNSegmentUniform(-1.0 * r, r, layerElem->wghtMat->rowNum * layerElem->wghtMat->colNum, layerElem->wghtMat->matElems);
	/* r = 0.22 for a (1000, 1000) layer; r = 0.083 for a (12000, 2000) layer */
        break;
    }

    /*if (layerElem->actfunKind == RELAF || layerElem->actfunKind == SOFTRELAF) {
        RandMaskNSegment(0.25, 0.0, layerElem->wghtMat->rowNum * layerElem->wghtMat->colNum, layerElem->wghtMat->matElems);
    }*/

    ClearNVector(layerElem->biasVec);
    /* TODO: if HERMITEAF */
#ifdef CUDA
    SyncNMatrixHost2Dev(layerElem->wghtMat);
    SyncNVectorDev2Host(layerElem->biasVec);
#endif

}

/* generate a new ANN layer and randomise it */
/*LELink GenRandLayer(MemHeap *heap, int nodeNum, int inputDim, int seed) {*/
LELink GenNewLayer(MemHeap *heap, int nodeNum, int inputDim) {
     LELink layerElem;

     /*layerElem = (LELink) New(heap, sizeof(LayerElem));*/
     layerElem = GenBlankLayer(heap);
     /*layerElem->operKind = operKind;
     layerElem->actfunKind = actfunKind;*/
     layerElem->nodeNum = nodeNum;
     layerElem->inputDim = inputDim;
     layerElem->wghtMat = CreateNMatrix(heap, nodeNum, inputDim);
     layerElem->biasVec = CreateNVector(heap, nodeNum);

     /*RandANNLayer(layerElem, seed);*/

     return layerElem;     
}

void SetFeaMixBatchIdxes(ANNSet *annSet, int newIdx) {
    int i;
    AILink curAI;
    ADLink annDef;
    LELink layerElem;
    NMatrix *dyFeaMat;

    /* init the ANNInfo pointer */
    curAI = annSet->defsTail;
    while (curAI != NULL) {
        annDef = curAI->annDef;
        for (i = annDef->layerNum - 1; i >= 0; --i) {
            layerElem = annDef->layerList[i];
	    if (layerElem->feaMix->batIdx == 0) {
                layerElem->feaMix->batIdx = newIdx; 
            }
        }
        curAI = curAI->next;
    }
}

/* cz277 - max norm2 */
Boolean IsLinearActFun(ActFunKind actfunKind) {
    switch (actfunKind) {
        case HERMITEAF:
        case LINEARAF:
        case RELUAF:
        case SOFTRELUAF:
            return TRUE;
        default:
            return FALSE;
    }
}

/* cz277 - max norm2 */
Boolean IsNonLinearActFun(ActFunKind actfunKind) {
    switch (actfunKind) {
        case SIGMOIDAF:
        case SOFTMAXAF:
        case SOFTSIGNAF:
        case TANHAF:
            return TRUE;
        default:
            return FALSE;
    }
}

/* cz277 - xform */
char *MakeRplNMatName(char *curSpkr, int rplIdx, char *rplName) {
    char buf[MAXSTRLEN];

    strcpy(rplName, curSpkr);
    strcat(rplName, "-");
    sprintf(buf, "%d", rplIdx);
    strcat(rplName, buf);
    strcat(rplName, RPLNMATSUFFIX);

    return rplName;
}

/* cz277 - xform */
char *MakeRplNVecName(char *curSpkr, int rplIdx, char *rplName) {
    char buf[MAXSTRLEN];

    strcpy(rplName, curSpkr);
    strcat(rplName, "-");
    sprintf(buf, "%d", rplIdx);
    strcat(rplName, buf);
    strcat(rplName, RPLNVECSUFFIX);

    return rplName;
}

/* cz277 - xform */
void ResetRplParts(ANNSet *annSet) {
    int i;
    RPLInfo *rplInfo;

    for (i = 1; i < MAXNUMRPLNMAT; ++i) {
        rplInfo = annSet->rplNMatInfo[i];
        if (rplInfo != NULL && rplInfo->inActive == TRUE) {
            if (rplInfo->rplNMat->matElems != rplInfo->saveRplNMatHost.matElems) {      /* speaker changed */
#ifdef CUDA
                SyncNMatrixDev2Host(rplInfo->rplNMat);  /* synchrnoise previous GPU updates to CPU */
#endif
                rplInfo->rplNMat->matElems = rplInfo->saveRplNMatHost.matElems;
#ifdef CUDA
                SyncNMatrixHost2Dev(rplInfo->rplNMat);  /* refresh GPU device*/
#endif
            }
        } 
    }
    for (i = 1; i < MAXNUMRPLNVEC; ++i) {
        rplInfo = annSet->rplNVecInfo[i];
        if (rplInfo != NULL && rplInfo->inActive == TRUE) {
            if (rplInfo->rplNVec->vecElems != rplInfo->saveRplNVecHost.vecElems) {      /* speaker changed */
#ifdef CUDA
                SyncNVectorDev2Host(rplInfo->rplNVec);  /* synchrnoise previous GPU updates to CPU */
#endif
                rplInfo->rplNVec->vecElems = rplInfo->saveRplNVecHost.vecElems;
#ifdef CUDA
                SyncNVectorHost2Dev(rplInfo->rplNVec);  /* refresh GPU device */
#endif
            }
        }   
    }
}

/* cz277 - pact */
int GetActFunNParmVector(LELink layerElem) {

    switch (layerElem->actFunInfo.actFunKind) {
    case AFFINEAF:
        return 2;
    case HERMITEAF:
        if (layerElem->actFunInfo.actParmVec[0] != NULL)
            return (-1) * ((int) layerElem->actFunInfo.actParmVec[0]->vecElems[0]);
        return ACTFUNPARMNOTLOAD;
    case LHUCPLRELUAF:
        return 2;
    case LHUCRELUAF:
        return 1;
    case LHUCSIGMOIDAF:
        return 1;
    case LHUCSOFTRELUAF:
        return 1;
    case PASIGMOIDAF:
        return 1;
    case PABSIGMOIDAF:
        return 2;
    case PBSIGMOIDAF:
        return 1;
    case PABCSIGMOIDAF:
        return 3;
    case PLRELUAF:
        return 1;
    case POLYLINEARAF:
        return 2;
    case PRELUAF:
        return 1;
    default:
        return 0;
    }
}

/* cz277 - pact */
Boolean CacheActMatrixOrNot(ActFunKind actFunKind) {
    switch (actFunKind) {
    case PABSIGMOIDAF:
    case PBSIGMOIDAF:
    case PABCSIGMOIDAF:
    case PABCSOFTRELUAF:
    case PLRELUAF:
    case POLYLINEARAF:
    case PRELUAF:
        return TRUE;
    default:
        return FALSE;
    }
}



